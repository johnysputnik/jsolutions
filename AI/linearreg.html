<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<title>Linear Regression</title>
<!-- 2016-01-23 Sat 14:44 -->
<meta  http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta  name="generator" content="Org-mode" />
<meta  name="author" content="John Cumming" />
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  .title  { text-align: center; }
  .todo   { font-family: monospace; color: red; }
  .done   { color: green; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #ccc;
    box-shadow: 3px 3px 3px #eee;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: visible;
    padding-top: 1.2em;
  }
  pre.src:before {
    display: none;
    position: absolute;
    background-color: white;
    top: -10px;
    right: 10px;
    padding: 3px;
    border: 1px solid black;
  }
  pre.src:hover:before { display: inline;}
  pre.src-sh:before    { content: 'sh'; }
  pre.src-bash:before  { content: 'sh'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-R:before     { content: 'R'; }
  pre.src-perl:before  { content: 'Perl'; }
  pre.src-java:before  { content: 'Java'; }
  pre.src-sql:before   { content: 'SQL'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.right  { text-align: center;  }
  th.left   { text-align: center;   }
  th.center { text-align: center; }
  td.right  { text-align: right;  }
  td.left   { text-align: left;   }
  td.center { text-align: center; }
  dt { font-weight: bold; }
  .footpara:nth-child(2) { display: inline; }
  .footpara { display: block; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  /*]]>*/-->
</style>
<link rel="stylesheet"
                         href="/css/solarized-dark.css"
                         type="text/css"/>
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2013 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
<script type="text/javascript" src="http://orgmode.org/mathjax/MathJax.js"></script>
<script type="text/javascript">
<!--/*--><![CDATA[/*><!--*/
    MathJax.Hub.Config({
        // Only one of the two following lines, depending on user settings
        // First allows browser-native MathML display, second forces HTML/CSS
        //  config: ["MMLorHTML.js"], jax: ["input/TeX"],
            jax: ["input/TeX", "output/HTML-CSS"],
        extensions: ["tex2jax.js","TeX/AMSmath.js","TeX/AMSsymbols.js",
                     "TeX/noUndefined.js"],
        tex2jax: {
            inlineMath: [ ["\\(","\\)"] ],
            displayMath: [ ['$$','$$'], ["\\[","\\]"], ["\\begin{displaymath}","\\end{displaymath}"] ],
            skipTags: ["script","noscript","style","textarea","pre","code"],
            ignoreClass: "tex2jax_ignore",
            processEscapes: false,
            processEnvironments: true,
            preview: "TeX"
        },
        showProcessingMessages: true,
        displayAlign: "center",
        displayIndent: "2em",

        "HTML-CSS": {
             scale: 100,
             availableFonts: ["STIX","TeX"],
             preferredFont: "TeX",
             webFont: "TeX",
             imageFont: "TeX",
             showMathMenu: true,
        },
        MMLorHTML: {
             prefer: {
                 MSIE:    "MML",
                 Firefox: "MML",
                 Opera:   "HTML",
                 other:   "HTML"
             }
        }
    });
/*]]>*///-->
</script>
</head>
<body>
<div id="preamble" class="status">
<div id="my-header"><h2>jSolutions | <a href="/index.html">home</a></h2></div>
</div>
<div id="content">
<h1 class="title">Linear Regression</h1>
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#sec-1">Introduction</a></li>
<li><a href="#sec-2">Univariate Linear Regression</a>
<ul>
<li><a href="#sec-2-1">Hypothesis Function</a></li>
<li><a href="#sec-2-2">Cost Function</a></li>
<li><a href="#sec-2-3">Gradient Descent Equation</a></li>
</ul>
</li>
<li><a href="#sec-3">Multivariate Linear Regression</a>
<ul>
<li><a href="#sec-3-1">Hypothesis Function</a></li>
<li><a href="#sec-3-2">Cost Function</a></li>
<li><a href="#sec-3-3">Gradient Descent Equation</a></li>
<li><a href="#sec-3-4">Normal Equation</a></li>
</ul>
</li>
<li><a href="#sec-4">Multivariate Linear Regression with Scilab</a></li>
</ul>
</div>
</div>

<div id="outline-container-sec-1" class="outline-2">
<h2 id="sec-1">Introduction</h2>
<div class="outline-text-2" id="text-1">
<p>
Machine learning problems that are classified as regression problems
are taking input variables and mapping the output to a continuous
expected result function. 
</p>
</div>
</div>

<div id="outline-container-sec-2" class="outline-2">
<h2 id="sec-2">Univariate Linear Regression</h2>
<div class="outline-text-2" id="text-2">
<p>
These notes provide the basic solution for
a linear regression with a single variable. Otherwise known as
univariate linear regression.
</p>
</div>

<div id="outline-container-sec-2-1" class="outline-3">
<h3 id="sec-2-1">Hypothesis Function</h3>
<div class="outline-text-3" id="text-2-1">
<p>
The hypothesis function is a linear equation that we use to
estimate the output function. We use a series of steps later to
resolve this function to within acceptable limits:
</p>

<p>
$$h_0(x) = \theta_0 + \theta_1x$$
</p>
</div>
</div>

<div id="outline-container-sec-2-2" class="outline-3">
<h3 id="sec-2-2">Cost Function</h3>
<div class="outline-text-3" id="text-2-2">
<p>
We can make intial guesses for the parameters of the hypothesis
function and use a cost function to measure the accuracy:
</p>

<p>
$$J(\theta_0, \theta_1) =
   \frac{1}{2m}\sum_{i=1}^m(h_{\theta}(x^{(i)}) - y^{(i)})^2$$
</p>
</div>
</div>

<div id="outline-container-sec-2-3" class="outline-3">
<h3 id="sec-2-3">Gradient Descent Equation</h3>
<div class="outline-text-3" id="text-2-3">
<p>
Now that we have both an estimate in the form of an hypothesis
function and a means to measure it's accuracy using the cost
function, we need a way to automatically improve our hypothesis
function. For this we use a gradient descent equation which, for
the case of the linear regression simplifies to the following 2
equations.
</p>

<p>
$$\theta_0 := \theta_0 -
   \alpha\frac{1}{m}\sum_{i=1}^m(h_{\theta}(x^{(i)}) - y^{(i)})$$
</p>

<p>
$$\theta_1 := \theta_1 -
   \alpha\frac{1}{m}\sum_{i=1}^m((h_{\theta}(x^{(i)}) - y^{(i)})x^{(i)})$$
</p>

<p>
alpha is the learning rate of the gradient descent equation, and m
is the size of the training set.
</p>

<p>
We can now repeat the gradient descent equation until the solution
converges within an acceptable limit.
</p>
</div>
</div>
</div>

<div id="outline-container-sec-3" class="outline-2">
<h2 id="sec-3">Multivariate Linear Regression</h2>
<div class="outline-text-2" id="text-3">
<p>
These notes provide the basic solution for a linear regression with
multiple variable. Otherwise known as multivariate linear
regression.
</p>
</div>

<div id="outline-container-sec-3-1" class="outline-3">
<h3 id="sec-3-1">Hypothesis Function</h3>
<div class="outline-text-3" id="text-3-1">
<p>
The hypothesis function is a linear equation that we use to
estimate the output function. We use a series of steps later to
resolve this function to within acceptable limits.
</p>

<p>
$$h_\theta(x) = \theta_0 + \theta_1x_1 + ... + \theta_nx_n$$
</p>
</div>
</div>

<div id="outline-container-sec-3-2" class="outline-3">
<h3 id="sec-3-2">Cost Function</h3>
<div class="outline-text-3" id="text-3-2">
<p>
We can make intial guesses for the parameters of the hypothesis
function and use a cost function to measure the accuracy:
</p>

<p>
$$J(\theta) =
   \frac{1}{2m}\sum_{i=1}^m(h_{\theta}(x^{(i)}) - y^{(i)})^2$$
</p>

<p>
This can be vectorised as follows:
</p>

<p>
$$J(\theta) = \frac{1}{2m}(X\theta - \vec{y}q)^T(X\theta - \vec{y})$$
</p>
</div>
</div>

<div id="outline-container-sec-3-3" class="outline-3">
<h3 id="sec-3-3">Gradient Descent Equation</h3>
<div class="outline-text-3" id="text-3-3">
<p>
Now that we have both an estimate in the form of an hypothesis
function and a means to measure it's accuracy using the cost
function, we need a way to automatically improve our hypothesis
function. For this we use a gradient descent equation which, for
the case of the linear regression simplifies to the following
vectorised equation.
</p>

<p>
$$\theta := \theta - \frac{\alpha}{m}X^T(X\theta - \vec{yq})$$
</p>

<p>
alpha is the learning rate of the gradient descent equation, and m
is the size of the training set.
</p>

<p>
We can now repeat the gradient descent equation until the solution
converges within an acceptable limit.
</p>
</div>
</div>

<div id="outline-container-sec-3-4" class="outline-3">
<h3 id="sec-3-4">Normal Equation</h3>
<div class="outline-text-3" id="text-3-4">
<p>
Alternatively, given a training set with the right variables, we
can solve the linear regression in a single step with the normal
equation:
</p>

<p>
$$\theta = (X^TX)^{-1}X^Ty$$
</p>

<p>
In order to do this the training set has to form a normal matrix.
</p>
</div>
</div>
</div>


<div id="outline-container-sec-4" class="outline-2">
<h2 id="sec-4">Multivariate Linear Regression with Scilab</h2>
<div class="outline-text-2" id="text-4">
<p>
This post outlines some code written to implement a simple
multivariate linear regression algorithm in Scilab. It was written
whilst I was completing a course on Machine Learning to be found on
Coursera. The course uses octave for most of it's examples but
I chose to write it in Scilab, for reasons not really worth going
into here.
</p>

<p>
First we must initialise the data from an input training set. The X
matrix contains the variables and the y vector contains the
results. The training set contains house prices based on house size
and number of bedrooms.
</p>

<div class="org-src-container">

<pre class="src src-octave"><span class="linenr">1: </span><span style="color: #859900;">//</span> Initialise data
<span class="linenr">2: </span>
<span class="linenr">3: </span>data <span style="color: #859900;">=</span> csvRead(<span style="color: #2aa198;">'ex1data2.txt'</span>)<span style="color: #859900;">;</span>
<span class="linenr">4: </span>X <span style="color: #859900;">=</span> data(<span style="color: #859900;">:,</span> 1<span style="color: #859900;">:</span>2)<span style="color: #859900;">;</span>
<span class="linenr">5: </span>y <span style="color: #859900;">=</span> data(<span style="color: #859900;">:,</span> 3)<span style="color: #859900;">;</span>
<span class="linenr">6: </span>m <span style="color: #859900;">=</span> length(y)
</pre>
</div>

<p>
Next the data is normalized according to a standard
distribution. This enables the algorithm to find convergence more
efficiently.
</p>

<div class="org-src-container">

<pre class="src src-octave"><span class="linenr"> 7: </span><span style="color: #859900;">//</span> normalize the data
<span class="linenr"> 8: </span>
<span class="linenr"> 9: </span>mu <span style="color: #859900;">=</span> mean(X<span style="color: #859900;">,</span> <span style="color: #2aa198;">"r"</span>)<span style="color: #859900;">;</span>
<span class="linenr">10: </span>sigma <span style="color: #859900;">=</span> stdev(X<span style="color: #859900;">,</span> <span style="color: #2aa198;">"r"</span>)<span style="color: #859900;">;</span>
<span class="linenr">11: </span>
<span class="linenr">12: </span><span style="color: #859900;">for</span> i <span style="color: #859900;">=</span> 1<span style="color: #859900;">:</span>size(X<span style="color: #859900;">,</span> <span style="color: #2aa198;">"c"</span>)
<span class="linenr">13: </span>  X(<span style="color: #859900;">:,</span>i) <span style="color: #859900;">=</span> (X(<span style="color: #859900;">:,</span>i) <span style="color: #859900;">-</span> mu(i)) <span style="color: #859900;">./</span> sigma(i)<span style="color: #859900;">;</span>
<span class="linenr">14: </span><span style="color: #859900;">end</span><span style="color: #859900;">;</span>
</pre>
</div>

<p>
Now that the training set variable matrix is normalized, we can add
the \(x_0\) variables of value 1, needed by the algorithm.
</p>

<div class="org-src-container">

<pre class="src src-octave"><span class="linenr">15: </span><span style="color: #859900;">//</span> add x_0 to X
<span class="linenr">16: </span>
<span class="linenr">17: </span>X <span style="color: #859900;">=</span> [ones(m<span style="color: #859900;">,</span> 1) X]<span style="color: #859900;">;</span>
</pre>
</div>

<p>
We can now set the parameters for the gradient descent calculations,
including the number of iterations and the linear equation
parameters, theta. We will also keep a record of the cost (J) at
each iteration, for displaying in a graph to ensure convergence to a
minimum.
</p>

<div class="org-src-container">

<pre class="src src-octave"><span class="linenr">18: </span><span style="color: #859900;">//</span> set parameters iterations and
<span class="linenr">19: </span><span style="color: #859900;">//</span> learning rate
<span class="linenr">20: </span>
<span class="linenr">21: </span>alpha <span style="color: #859900;">=</span> 0.01<span style="color: #859900;">;</span>
<span class="linenr">22: </span>num_iters <span style="color: #859900;">=</span> 400<span style="color: #859900;">;</span>
<span class="linenr">23: </span>theta <span style="color: #859900;">=</span> zeros(3<span style="color: #859900;">,</span> 1)<span style="color: #859900;">;</span>
<span class="linenr">24: </span>J_history <span style="color: #859900;">=</span> zeros(num_iters<span style="color: #859900;">,</span> 1)<span style="color: #859900;">;</span>
</pre>
</div>

<p>
We can now run the gradient descent algorithm to calculate values of
theta at convergence.
</p>

<div class="org-src-container">

<pre class="src src-octave"><span class="linenr">25: </span><span style="color: #859900;">for</span> iter <span style="color: #859900;">=</span> 1<span style="color: #859900;">:</span>num_iters
<span class="linenr">26: </span>  hypothesis <span style="color: #859900;">=</span> X <span style="color: #859900;">*</span> theta<span style="color: #859900;">;</span>
<span class="linenr">27: </span>  errors <span style="color: #859900;">=</span> hypothesis <span style="color: #859900;">-</span> y<span style="color: #859900;">;</span>
<span class="linenr">28: </span>  change <span style="color: #859900;">=</span> (alpha <span style="color: #859900;">*</span> (X<span style="color: #859900;">'</span> <span style="color: #859900;">*</span> errors)) <span style="color: #859900;">/</span> m<span style="color: #859900;">;</span>
<span class="linenr">29: </span>  theta <span style="color: #859900;">=</span> theta <span style="color: #859900;">-</span> change<span style="color: #859900;">;</span>
<span class="linenr">30: </span>
<span class="linenr">31: </span>  J_history(iter) <span style="color: #859900;">=</span> (sum(((X <span style="color: #859900;">*</span> theta) <span style="color: #859900;">-</span> y) <span style="color: #859900;">.^</span> 2)) <span style="color: #859900;">/</span> (2 <span style="color: #859900;">*</span> m)<span style="color: #859900;">;</span>
<span class="linenr">32: </span><span style="color: #859900;">end</span><span style="color: #859900;">;</span>
</pre>
</div>

<p>
Now that we have completed the gradient descent, we can plot the cost
value and check that it does actually converge.
</p>

<div class="org-src-container">

<pre class="src src-octave"><span class="linenr">33: </span><span style="color: #859900;">//</span> plot the convergence graph
<span class="linenr">34: </span>
<span class="linenr">35: </span>plot(1<span style="color: #859900;">:</span>size(J_history<span style="color: #859900;">,</span> <span style="color: #2aa198;">"r"</span>)<span style="color: #859900;">,</span> J_history<span style="color: #859900;">,</span> <span style="color: #2aa198;">'-g'</span><span style="color: #859900;">,</span> <span style="color: #2aa198;">'LineWidth'</span><span style="color: #859900;">,</span> 1)<span style="color: #859900;">;</span>
<span class="linenr">36: </span>xtitle(<span style="color: #2aa198;">"Convergence of Cost Function"</span><span style="color: #859900;">,</span> <span style="color: #2aa198;">"Iterations"</span><span style="color: #859900;">,</span> <span style="color: #2aa198;">"Cost"</span>)<span style="color: #859900;">;</span>
</pre>
</div>


<div class="figure">
<p><img src="multi_example_cost.png" alt="multi_example_cost.png" />
</p>
</div>

<p>
As we can see that the cost converges, we can now use the values of
theta (and the normalization parameters) to calculate predicted
house prices.
</p>

<div class="org-src-container">

<pre class="src src-octave"><span class="linenr">37: </span><span style="color: #859900;">//</span>  Estimate the price of a 1650 sq<span style="color: #859900;">-</span>ft<span style="color: #859900;">,</span> 3 br house
<span class="linenr">38: </span>
<span class="linenr">39: </span>norm_area <span style="color: #859900;">=</span> (1650 <span style="color: #859900;">-</span> mu(1)) <span style="color: #859900;">/</span> sigma(1)<span style="color: #859900;">;</span>
<span class="linenr">40: </span>norm_bedrooms <span style="color: #859900;">=</span> (3 <span style="color: #859900;">-</span> mu(2)) <span style="color: #859900;">/</span> sigma(2)<span style="color: #859900;">;</span>
<span class="linenr">41: </span>price <span style="color: #859900;">=</span> theta(1) <span style="color: #859900;">+</span> (theta(2) <span style="color: #859900;">*</span> norm_area) <span style="color: #859900;">+</span> (theta(3) <span style="color: #859900;">*</span> norm_bedrooms)<span style="color: #859900;">;</span>
</pre>
</div>

<p>
This code should scale nicely to large numbers of variables in the
linear equation and training data set.
</p>

<p>
The complete source code can be found <a href="lin-reg.sci">here</a>.
</p>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="author">Author: John Cumming</p>
<p class="date">Created: 2016-01-23 Sat 14:44</p>
<p class="creator"><a href="http://www.gnu.org/software/emacs/">Emacs</a> 24.5.1 (<a href="http://orgmode.org">Org</a> mode 8.2.10)</p>
<p class="validation"><a href="http://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
