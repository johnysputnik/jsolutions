#+TITLE: Basic Logical Regression

* Introduction

  Machine learning problems that are classified as classification
  problems are taking input variables and mapping the output to a
  discrete value. These notes provide the basic solution for a logical
  regression algorithm for a binary classification problem.

* Hypothesis Function

  The hypothesis function is an equation that we use to estimate the
  output function. We use a series of steps later to resolve this
  function to within acceptable limits.

$$0 \leq h_\theta(x) \leq 1$$

$$h_\theta(x) = g(\theta^Tx)$$

$$z = \theta^Tx$$

$$g(z) = \frac{1}{1+e^{-z}}$$

* Cost Function

  We can make initial guesses for the parameters of the hypothesis
  function and use a cost function to measure the accuracy:

$$J(\theta) = - \frac{1}{m}(\log(g(X\theta))^Ty +
log(1-g(X\theta))^T(1-y))$$

  Now that we have both an estimate in the form of an hypothesis
  function and a means to measure it's accuracy using the cost function,
  we need a way to automatically improve our hypothesis function.

* Gradient Descent Equation

  For this we use a gradient descent equation which, for the case of
  the linear regression simplifies to the following vectorised
  equation.

$$\theta := \theta - \frac{\alpha}{m}X^T(g(X\theta) - \vec{y}
)$$

  alpha is the learning rate of the gradient descent equation, and m
  is the size of the training set.

  We can now repeat the gradient descent equation until the solution
  converges within an acceptable limit.
